{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f643d500",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61fc951",
   "metadata": {},
   "source": [
    "Welcome to the interactive tracking notebook!\\\n",
    "This notebook goes through each step and allows you to tune parameters and view how it changes the results.\n",
    "\n",
    "The notebook proceeds as follows:\n",
    "1. **Import** libraries\n",
    "2. Define **paths** to data\n",
    "3. Run data through the **pipeline**. Each step of the pipeline is run by a single unique python class.\n",
    "4. **Visualize** results\n",
    "5. **Save** results\n",
    "\n",
    "As you go through the notebook, take note of the small number of parameters that are mentioned as **'important parameters'** (consider searching for these in the notebook). We consider these to be the only parameters that can have a large effect on the run output. Other parameters matter and should be considered as well, but are less critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50622a4e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9824970",
   "metadata": {},
   "source": [
    "Widen the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997af9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# widen jupyter notebook window\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cbc7a4",
   "metadata": {},
   "source": [
    "Import basic libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4eb1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import multiprocessing as mp\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24c78a",
   "metadata": {},
   "source": [
    "Import `roicat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db7f4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import roicat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8adf6d9",
   "metadata": {},
   "source": [
    "# Find paths to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8437c9",
   "metadata": {},
   "source": [
    "In this example we are using suite2p output files, but other data types can be used (CaImAn, etc.) \\\n",
    "See the notebook on ingesting diverse data: https://github.com/RichieHakim/ROICaT/blob/main/notebooks/jupyter/other/demo_data_importing.ipynb\n",
    "\n",
    "Make a list containing the paths to all the input files.\n",
    "\n",
    "In this example we are using suite2p, so the following are defined:\n",
    "1. `paths_allStat`: a list to all the stat.npy files\n",
    "2. `paths_allOps`: a list with ops.npy files that correspond 1-to-1 with the stat.npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de208487-2702-4bd7-96be-4e62e8948de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_allOuterFolders = r'/media/rich/bigSSD/analysis_data/face_rhythm/mouse_0322R/'\n",
    "\n",
    "pathSuffixToStat = 'stat.npy'\n",
    "pathSuffixToOps = 'ops.npy'\n",
    "\n",
    "paths_allStat = roicat.helpers.find_paths(\n",
    "    dir_outer=dir_allOuterFolders,\n",
    "    reMatch=pathSuffixToStat,\n",
    "    depth=6,\n",
    ")[:3]\n",
    "paths_allOps  = np.array([Path(path).resolve().parent / pathSuffixToOps for path in paths_allStat])[:]\n",
    "\n",
    "print(f'paths to all stat files:');\n",
    "[print(path) for path in paths_allStat];\n",
    "print('');\n",
    "print(f'paths to all ops files:');\n",
    "[print(path) for path in paths_allOps];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ea98d",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f7143",
   "metadata": {},
   "source": [
    "**Important parameters**:\n",
    "- `um_per_pixel` (float):\n",
    "    - Resolution. 'micrometers per pixel' of the imaging field of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19a92de-f36e-4d89-ac5c-65394d362d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = roicat.data_importing.Data_suite2p(\n",
    "    paths_statFiles=paths_allStat[:],\n",
    "    paths_opsFiles=paths_allOps[:],\n",
    "    um_per_pixel=1.0,  ## IMPORTANT PARAMETER. Use a list of floats if values differ in each session.\n",
    "    new_or_old_suite2p='new',\n",
    "    type_meanImg='meanImgE',\n",
    "#     FOV_images=FOVs_mixed,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "assert data.check_completeness(verbose=False)['tracking'], f\"Data object is missing attributes necessary for tracking.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fe236-4e97-4cfe-8df5-16a99ed51acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "roicat.visualization.display_toggle_image_stack(data.get_maxIntensityProjection_spatialFootprints(), clim=[0,1])\n",
    "roicat.visualization.display_toggle_image_stack(np.concatenate(data.ROI_images, axis=0)[:5000], image_size=(200,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6657766",
   "metadata": {},
   "source": [
    "# Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7e32c",
   "metadata": {},
   "source": [
    "This is the most important step in the pipeline to stop and check that everything looks okay and tune parameters if necessary. I strongly recommend reading the documentation for the methods for the `roicat.tracking.alignment.Aligner` class at each step.\n",
    "\n",
    "Alignment is 4 steps:\n",
    "\n",
    "1. FOV_image augmentation\n",
    "2. Fit geometric transformation\n",
    "3. Fit non-rigid transformation (on top of the geometric)\n",
    "4. Apply transformation to ROIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618c2df",
   "metadata": {},
   "source": [
    "##### 1. FOV_image augmentation\n",
    "Do what is necessary to make the augmented FOV_images look good. Use the visualization tool below to help. This can include playing with the mixing factor, normalization, and playing with the CLAHE parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56ce51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner = roicat.tracking.alignment.Aligner(verbose=True)\n",
    "\n",
    "FOV_images = aligner.augment_FOV_images(\n",
    "    FOV_images=data.FOV_images,\n",
    "    spatialFootprints=data.spatialFootprints,\n",
    "    normalize_FOV_intensities=True,\n",
    "    roi_FOV_mixing_factor=0.5,\n",
    "    use_CLAHE=True,\n",
    "    CLAHE_grid_size=100,\n",
    "    CLAHE_clipLimit=1,\n",
    "    CLAHE_normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79722a42",
   "metadata": {},
   "source": [
    "View the augmented FOV images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd401a-d988-4ada-aac7-fecb00e82ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(FOV_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658db1af",
   "metadata": {},
   "source": [
    "##### 2. Fit geometric transformation\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help. Please consider reading the documentation if you have any issues with the alignment step (TODO: link to readthedocs) for more details.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template`=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e59dd-f280-4bc2-9264-937bac0c89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_geometric(\n",
    "#     template=FOV_images[4],\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=FOV_images,  ## input images\n",
    "    template_method='sequential',  ## 'sequential': align images to neighboring images (good for drifting data). 'image': align to a single image\n",
    "    mode_transform='euclidean',  ## type of geometric transformation. See openCV's cv2.findTransformECC for details\n",
    "    mask_borders=(50,50,50,50),  ## number of pixels to mask off the edges (top, bottom, left, right)\n",
    "    n_iter=50,  ## number of iterations for optimization\n",
    "    termination_eps=1e-09,  ## convergence tolerance\n",
    "    gaussFiltSize=31,  ## size of gaussian blurring filter applied to all images\n",
    "    auto_fix_gaussFilt_step=10,  ## increment in gaussFiltSize after a failed optimization\n",
    ")\n",
    "\n",
    "aligner.transform_images_geometric(FOV_images);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54ce48",
   "metadata": {},
   "source": [
    "##### 3. Fit non-rigid transformation\n",
    "Play with parameters until the aligned FOV_images look good. The visualization tool below can help.\n",
    "\n",
    "We like the following **important parameters**:\n",
    "- `template`=0.5\n",
    "- `template_method`='image'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a639a0c-2dda-483d-ab87-d9a7fb769420",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.fit_nonrigid(\n",
    "#     template=FOV_images[1],\n",
    "    template=0.5,  ## specifies which image to use as the template. Either array (image), integer (ims_moving index), or float (ims_moving fractional index)\n",
    "    ims_moving=aligner.ims_registered_geo,  ## Input images. Typically the geometrically registered images\n",
    "    remappingIdx_init=aligner.remappingIdx_geo,  ## The remappingIdx between the original images (and ROIs) and ims_moving\n",
    "    template_method='image',  ## 'sequential': align images to neighboring images. 'image': align to a single image, good if using geometric registration first\n",
    "    mode_transform='createOptFlow_DeepFlow',  ## algorithm for non-rigid transformation. Either 'createOptFlow_DeepFlow' or 'calcOpticalFlowFarneback'. See openCV docs for each. \n",
    "    kwargs_mode_transform=None,  ## kwargs for `mode_transform`\n",
    ")\n",
    "\n",
    "aligner.transform_images_nonrigid(FOV_images);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e24834b",
   "metadata": {},
   "source": [
    "##### 4. Transform ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbf309-6064-4539-87de-72a59d1863dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligner.transform_ROIs(\n",
    "    ROIs=data.spatialFootprints, \n",
    "    remappingIdx=aligner.remappingIdx_nonrigid,\n",
    "    normalize=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054fb0a",
   "metadata": {},
   "source": [
    "Ensure that the aligned FOVs look aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd09701-7df7-4af5-a8d8-1fed52c7d4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pre-alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(data.FOV_images)\n",
    "print(f'Geometric alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_geo)\n",
    "print(f'Non-rigid alignment below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.ims_registered_nonrigid)\n",
    "print(f'Transformed ROIs below')\n",
    "roicat.visualization.display_toggle_image_stack(aligner.get_ROIsAligned_maxIntensityProjection(normalize=True), clim=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce87c7",
   "metadata": {},
   "source": [
    "# Blur ROIs\n",
    "\n",
    "ROIs from different sessions with zero spatial overlap have very low probability of being considered the same ROI during the clustering step. Blurring the spatial footprint masks can increase the overlap between ROIs that drift apart from each other. It's a good idea to increase the `kernel_halfWidth` if you are working with sparsely labeled ROIs or ROIs that change/move from session to session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b54df-c744-44fb-9cb4-a124e4a0694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "blurrer = roicat.tracking.blurring.ROI_Blurrer(\n",
    "    frame_shape=(data.FOV_height, data.FOV_width),  ## FOV height and width\n",
    "    kernel_halfWidth=6,  ## The half width of the 2D gaussian used to blur the ROI masks\n",
    "    plot_kernel=False,  ## Whether to visualize the 2D gaussian\n",
    ")\n",
    "\n",
    "blurrer.blur_ROIs(\n",
    "    spatialFootprints=aligner.ROIs_aligned[:],\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a9af1",
   "metadata": {},
   "source": [
    "See that the blurred ROIs are overlapping each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffb59a-6b39-4866-8c71-278a08647e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(blurrer.get_ROIsBlurred_maxIntensityProjection())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8778709",
   "metadata": {},
   "source": [
    "# ROInet embedding\n",
    "\n",
    "This step passes the images of each ROI through the ROInet neural network. The inputs are the images, the output is an array describing the visual properties of each ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6e7d80",
   "metadata": {},
   "source": [
    "Initialize the ROInet object. The `ROInet_embedder` class will automatically download and load a pretrained ROInet model. If you have a GPU, this step will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbed81b-1e36-4c1b-b8d1-b2f519d54f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = roicat.helpers.set_device(use_GPU=True, verbose=True)\n",
    "dir_temp = tempfile.gettempdir()\n",
    "\n",
    "roinet = roicat.ROInet.ROInet_embedder(\n",
    "    device=DEVICE,  ## Which torch device to use ('cpu', 'cuda', etc.)\n",
    "    dir_networkFiles=dir_temp,  ## Directory to download the pretrained network to\n",
    "    download_method='check_local_first',  ## Check to see if a model has already been downloaded to the location (will skip if hash matches)\n",
    "    download_url='https://osf.io/x3fd2/download',  ## URL of the model\n",
    "    download_hash='7a5fb8ad94b110037785a46b9463ea94',  ## Hash of the model file\n",
    "    forward_pass_version='latent',  ## How the data is passed through the network\n",
    "    verbose=True,  ## Whether to print updates\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395651ed",
   "metadata": {},
   "source": [
    "Resize ROIs and prepare a dataloader.\n",
    "\n",
    "**Important parameters**:\n",
    "- `um_per_pixel`: (same as specified in `data` object). Resolution of FOV. This is used to resize the ROIs to be relatively consistent across resolutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd19de0-5c42-4add-8be2-a7fe4ae6989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_dataloader(\n",
    "    ROI_images=data.ROI_images,  ## Input images of ROIs\n",
    "    um_per_pixel=data.um_per_pixel,  ## Resolution of FOV\n",
    "    pref_plot=False,  ## Whether or not to plot the ROI sizes\n",
    "    \n",
    "    jit_script_transforms=False,  ## (advanced) Whether or not to use torch.jit.script to speed things up\n",
    "    \n",
    "    batchSize_dataloader=8,  ## (advanced) PyTorch dataloader batch_size\n",
    "    pinMemory_dataloader=True,  ## (advanced) PyTorch dataloader pin_memory\n",
    "    numWorkers_dataloader=mp.cpu_count(),  ## (advanced) PyTorch dataloader num_workers\n",
    "    persistentWorkers_dataloader=True,  ## (advanced) PyTorch dataloader persistent_workers\n",
    "    prefetchFactor_dataloader=2,  ## (advanced) PyTorch dataloader prefetch_factor\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df35f7",
   "metadata": {},
   "source": [
    "In general, you want to see that a neuron fills roughly 25-50% of the area of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333e420-3029-4ae3-b484-53e6baa4fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_toggle_image_stack(roinet.ROI_images_rs[:1000], image_size=(200,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4a594",
   "metadata": {},
   "source": [
    "Pass the data through the network. Expect for large datasets (~40,000 ROIs) that this takes around 15 minutes on CPU or 1 minute on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f996ab-1514-4e90-923b-f26b55fdc27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roinet.generate_latents();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26152bd",
   "metadata": {},
   "source": [
    "# Scattering wavelet embedding\n",
    "\n",
    "This is similar to the ROInet embedding in purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806b1cd-bf59-4cfb-9741-7c2c1269b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "swt = roicat.tracking.scatteringWaveletTransformer.SWT(\n",
    "    kwargs_Scattering2D={'J': 2, 'L': 12},  ## 'J' is the number of convolutional layers. 'L' is the number of wavelet angles.\n",
    "    image_shape=data.ROI_images[0].shape[1:3],  ## size of a cropped ROI image\n",
    "    device=DEVICE,  ## PyTorch device\n",
    ")\n",
    "\n",
    "swt.transform(\n",
    "    ROI_images=roinet.ROI_images_rs,  ## All the cropped and resized ROI images\n",
    "    batch_size=100,  ## Batch size for each iteration (smaller is less memory but slower)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc0d2b",
   "metadata": {},
   "source": [
    "# Compute similarities\n",
    "\n",
    "Now we can compare the similarities of the ROIs. This includes calculating 4 kinds of similarities:\n",
    "1. `s_sf`: 'similarity spatial footprint'. The physical overlap between ROIs.\n",
    "2. `s_NN`: 'similarity neural network'. The similarities of the embeddings out of ROInet.\n",
    "3. `s_SWT`: 'similarity scaterring wavelet transform'. The similarities of the embeddings out of the scattering wavelet transformer.\n",
    "4. `s_sesh`: 'similarity sessions'. 0 if from the same session, 1 if from different sessions. ROIs from the same session have 0 probability of being the same.\n",
    "\n",
    "The result of this step will be a set of pairwise similarity matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be787416",
   "metadata": {},
   "source": [
    "Initialize the `ROI_graph` class and compute similarities.\n",
    "To make computation more efficient, only ROIs within the same 'block' are compared against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5d1e5-67f7-4195-b009-7aa10cd45f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = roicat.tracking.similarity_graph.ROI_graph(\n",
    "    n_workers=-1,  ## Number of CPU cores to use. -1 for all.\n",
    "    frame_height=data.FOV_height,\n",
    "    frame_width=data.FOV_width,\n",
    "    block_height=128,  ## size of a block\n",
    "    block_width=128,  ## size of a block\n",
    "    algorithm_nearestNeigbors_spatialFootprints='brute',  ## algorithm used to find the pairwise similarity for s_sf. ('brute' is slow but exact. See docs for others.)\n",
    "    verbose=True,  ## Whether to print outputs\n",
    ")\n",
    "\n",
    "sim.visualize_blocks()\n",
    "\n",
    "s_sf, s_NN, s_SWT, s_sesh = sim.compute_similarity_blockwise(\n",
    "    spatialFootprints=blurrer.ROIs_blurred,  ## Mask spatial footprints\n",
    "    features_NN=roinet.latents,  ## ROInet output latents\n",
    "    features_SWT=swt.latents,  ## Scattering wavelet transform output latents\n",
    "    ROI_session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "    spatialFootprint_maskPower=1.0,  ##  An exponent to raise the spatial footprints to to care more or less about bright pixels\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3861019",
   "metadata": {},
   "source": [
    "It is useful to normalize the similarity matrices using the local ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269de51-e506-41a9-b979-d8fb8c8bf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.make_normalized_similarities(\n",
    "    centers_of_mass=data.centroids,  ## ROI centroid positions\n",
    "    features_NN=roinet.latents,  ## ROInet latents\n",
    "    features_SWT=swt.latents,  ## SWT latents\n",
    "    k_max=data.n_sessions*100,  ## Maximum number of nearest neighbors to consider for the normalizing distribution\n",
    "    k_min=data.n_sessions*10,  ## Minimum number of nearest neighbors to consider for the normalizing distribution\n",
    "    algo_NN='kd_tree',  ## Nearest neighbors algorithm to use\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b02deb",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "This step does the following:\n",
    "1. Mix the similarity matrices into a single distance matrix\n",
    "2. Prune the distance matrix to remove low probability connections\n",
    "3. Perform clustering\n",
    "4. Compute quality metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14315d",
   "metadata": {},
   "source": [
    "#### 1. Mix the similarity matrices into a single distance matrix\n",
    "\n",
    "This step can be done either automatically, using the `clusterer.find_optimal_parameters_for_pruning` method, or manually by specifying the `kwargs_makeConjunctiveDistanceMatrix` dictionary. If you have a smaller total number of ROIs (<100 ROIs per session and/or <8 sessions), then it may be a good idea to manually play with the parameters in the next cell instead of using the automatic method.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "##### Option A: Automatic Method\n",
    "This step finds the optimal parameters to mix the similarity matrices by tuning mixing parameters to maximally separate the distributions of pairwise similarities for ROI pairs known to be different and ROI pairs that are likely matched.\n",
    "\n",
    "Some of the details of underlying algorithm:\n",
    "1. For each step in the optimization process, all the similarity matrices (`s_sf`, `s_NN_z`, `s_SWT_z`, `s_sesh`) are each passed through a sigmoid activation function that is parameterized (e.g. `'power_SF'`, `'sig_SF_kwargs'`), then all the similarity matrices are combined using a p-norm, where the 'p' is parameterized with `'p_norm'`. This results in a single conjunctive similarity matrix called `sConj` bounded between 0-1. \n",
    "2. `sConj` is converted into a distance matrix `dConj` (bounded from 1-0).\n",
    "3. `dConj` is then passed through the objective function: The full distance matrix is separated into a few components. First, all pairs of ROIs that are known to be from 'different' sources because they are from the same session are separated out into a distribution of pairwise distances (`d_diff`). Second, we define pairs of ROIs that are likely to be from the same source as `d_same` = `d_all` - `d_diff`. The objective function is then the overlap between the `d_diff` and `d_same` distributions. \n",
    "4. The objective function is minimized by tuning the mixing parameters in `kwargs_makeConjunctiveDistanceMatrix`.\n",
    "5. The output of this step is the optimal `kwargs_makeConjunctiveDistanceMatrix` dictionary.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "##### Option B: Manual Method\n",
    "You can also simply specify the `kwargs_makeConjunctiveDistanceMatrix` dictionary manually. This is useful if you have a good idea of what the optimal parameters are or if the automatic method is not working well. Uncomment the code block below to overwrite the `kwargs_mcdm_tmp` variable.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#### TROUBLESHOOTING FOR THIS STEP\n",
    "- If you have any issues, just email Rich Hakim or open an issue on the github issues page.\n",
    "- If you see: `'No crossover found, not plotting'`: Your data may not be easily separable. For some people, this is because the number of matching ROIs is very low compared to the number of non-matching ROIs. I recommend trying out the manual method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04f29fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the clusterer object by passing the similarity matrices in\n",
    "clusterer = roicat.tracking.clustering.Clusterer(\n",
    "    s_sf=sim.s_sf,\n",
    "    s_NN_z=sim.s_NN_z,\n",
    "    s_SWT_z=sim.s_SWT_z,\n",
    "    s_sesh=sim.s_sesh,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cfbb6e",
   "metadata": {},
   "source": [
    "#### Automatic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1ff7b-d6f9-4ba5-8a01-f8851cd10907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to automatically find mixing parameters\n",
    "kwargs_makeConjunctiveDistanceMatrix_best = clusterer.find_optimal_parameters_for_pruning(\n",
    "    n_bins=None,  ## Number of bins to use for the histograms of the distributions. If None, then a heuristic is used.\n",
    "    smoothing_window_bins=None,  ## Number of bins to use to smooth the distributions. If None, then a heuristic is used.\n",
    "    kwargs_findParameters={\n",
    "        'n_patience': 300,  ## Number of optimization epoch to wait for tol_frac to converge\n",
    "        'tol_frac': 0.001,  ## Fractional change below which optimization will conclude\n",
    "        'max_trials': 120,  ## Max number of optimization epochs\n",
    "        'max_duration': 60*10,  ## Max amount of time (in seconds) to allow optimization to proceed for\n",
    "        'value_stop': 0.0,  ## Goal value. If value equals or goes below value_stop, optimization is stopped.\n",
    "    },\n",
    "    bounds_findParameters={\n",
    "        'power_NN': (0.0, 2.),  ## Bounds for the exponent applied to s_NN\n",
    "        'power_SWT': (0.0, 2.),  ## Bounds for the exponent applied to s_SWT\n",
    "        'p_norm': (-5, -0.1),  ## Bounds for the p-norm p value (Minkowski) applied to mix the matrices\n",
    "        'sig_NN_kwargs_mu': (0., 1.0),  ## Bounds for the sigmoid center for s_NN\n",
    "        'sig_NN_kwargs_b': (0.1, 1.5),  ## Bounds for the sigmoid slope for s_NN\n",
    "        'sig_SWT_kwargs_mu': (0., 1.0),  ## Bounds for the sigmoid center for s_SWT\n",
    "        'sig_SWT_kwargs_b': (0.1, 1.5),  ## Bounds for the sigmoid slope for s_SWT\n",
    "    },\n",
    "    n_jobs_findParameters=-1,  ## Number of CPU cores to use (-1 is all cores)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d62119",
   "metadata": {},
   "source": [
    "#### Manual method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d439825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment below to manually specify mixing parameters\n",
    "# kwargs_makeConjunctiveDistanceMatrix_best = {\n",
    "#     'power_SF': 1.0,   ## s_sf**power_SF   (Higher values means clustering is more sensitive to spatial overlap of ROIs)\n",
    "#     'power_NN': 4.988104678381475,   ## s_NN**power_NN   (Higher values means clustering is more sensitive to visual similarity of ROIs)\n",
    "#     'power_SWT': 3.2306101591115177,  ## s_SWT**power_SWT (Higher values means clustering is more sensitive to visual similarity of ROIs)\n",
    "#     'p_norm': -3.7239759637888254,    ## norm([s_sf, s_NN, s_SWT], p=p_norm) (Higher values means clustering requires all similarity metrics to be high)\n",
    "# #     'sig_SF_kwargs': {'mu':0.5, 'b':1.0},  ## Sigmoid parameters for s_sf (mu is the center, b is the slope)\n",
    "#     'sig_SF_kwargs': None,\n",
    "#     'sig_NN_kwargs': {'mu':0.022482651499435957, 'b':0.02659655318391102},    ## Sigmoid parameters for s_NN (mu is the center, b is the slope)\n",
    "# #     'sig_NN_kwargs': None,\n",
    "#     'sig_SWT_kwargs': {'mu':0.10929602726304388, 'b':0.25801625013167434}, ## Sigmoid parameters for s_SWT (mu is the center, b is the slope)\n",
    "# #     'sig_SWT_kwargs': None,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83cd43e",
   "metadata": {},
   "source": [
    "#### View mixing results\n",
    "The goal is to see a **bimodal curve** in the pairwise similarities and a **clear cross-over point** (specified by the vertical dotted line) between 'same' and 'diff' pairs of ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004e69f-e4dc-48e7-8340-e4fd6eec0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.plot_distSame(kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best)\n",
    "\n",
    "clusterer.plot_similarity_relationships(\n",
    "    plots_to_show=[1,2,3], \n",
    "    max_samples=100000,  ## Make smaller if it is running too slow\n",
    "    kwargs_scatter={'s':1, 'alpha':0.2},\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2511098",
   "metadata": {},
   "source": [
    "##### 2. Prune the distance matrix\n",
    "\n",
    "We can remove all connections in the distance graph with probabilities of connection of less than 50%. We estimate this cutoff distance as the cross-over point between the 'same' and 'different' distributions.\n",
    "\n",
    "**Important parameter**\\\n",
    "`stringency`: This value changes the threshold for pruning the distance matrix. A higher value will result in less pruning, and a lower value will result in more pruning. The value will be multiplied by the inferred threshold to get the new one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed79a9-b95f-4670-bf91-31ae65235898",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer.make_pruned_similarity_graphs(\n",
    "    d_cutoff=None,  ## Optionally manually specify a distance cutoff\n",
    "    kwargs_makeConjunctiveDistanceMatrix=kwargs_makeConjunctiveDistanceMatrix_best,\n",
    "    stringency=1.0,  ## Modifies the threshold for pruning the distance matrix. Higher values result in LESS pruning. New d_cutoff = stringency * truncated d_cutoff.\n",
    "    convert_to_probability=False,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f87265",
   "metadata": {},
   "source": [
    "##### 3. Cluster\n",
    "\n",
    "There are two methods for clustering: The standard method `.fit` which is based on HDBSCAN, and `.fit_sequentialHungarian` which is an algorithm that is also used by CaImAn based on the Hungarian algorithm. The standard method takes 1-20 minutes and works better when there are many sessions, the Hungarian method takes seconds and works better when there are fewer sessions (<8).\n",
    "\n",
    "**Important parameters**:\n",
    "- For standard **`.fit`** method:\n",
    "1. `min_cluster_size`: If you only want ROIs clusters with at least a certain number of samples, specify here.\n",
    "2. `n_iter_violationCorrection`: This parameter controls how fast this step takes. Turning it down has mild effects on quality. We use around ***6***.\n",
    "\n",
    "- For **`.fit_sequentialHungarian`** method:\n",
    "1. `thresh_cost`: Determines the threshold of how distant two ROIs can be and still be matched. Smaller value is more stringent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db56330-e5e4-4400-ad7a-e63934a2eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data.n_sessions >= 6:\n",
    "    labels = clusterer.fit(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        min_cluster_size=2,  ## Minimum number of ROIs that can be considered a 'cluster'\n",
    "        n_iter_violationCorrection=6,  ## Number of times to redo clustering sweep after removing violations\n",
    "        split_intraSession_clusters=True,  ## Whether or not to split clusters with ROIs from the same session\n",
    "        cluster_selection_method='leaf',  ## (advanced) Method of cluster selection for HDBSCAN (see hdbscan documentation)\n",
    "        d_clusterMerge=None,  ## Distance below which all ROIs are merged into a cluster\n",
    "        alpha=0.999,  ## (advanced) Scalar applied to distance matrix in HDBSCAN (see hdbscan documentation)\n",
    "        discard_failed_pruning=True,  ## (advanced) Whether or not to set all ROIs that could be separated from clusters with ROIs from the same sessions to label=-1\n",
    "        n_steps_clusterSplit=100,  ## (advanced) How finely to step through distances to remove violations\n",
    "    )\n",
    "\n",
    "else:\n",
    "    labels = clusterer.fit_sequentialHungarian(\n",
    "        d_conj=clusterer.dConj_pruned,  ## Input distance matrix\n",
    "        session_bool=data.session_bool,  ## Boolean array of which ROIs belong to which sessions\n",
    "        thresh_cost=0.8,  ## Threshold. Higher values result in more permissive clustering. Specifically, the pairwise metric distance between ROIs above which two ROIs cannot be clustered together.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106399e4",
   "metadata": {},
   "source": [
    "##### 4. Quality metrics\n",
    "\n",
    "Compute various quality scores for each cluster and each ROI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b4f12-1ea5-4487-9188-504991ad272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKIP THIS STEP FOR VERY LARGE DATASETS\n",
    "quality_metrics = clusterer.compute_quality_metrics();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f8c1c",
   "metadata": {},
   "source": [
    "## Collect results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9234c",
   "metadata": {},
   "source": [
    "1. Make different versions of the labels for convenience.\n",
    "2. Put all the useful results and info into a dictionary to save later. ADJUST THIS ANY WAY YOU WANT.\n",
    "3. Put all the class objects from the run into a dictionary to save later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd22cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_squeezed, labels_bySession, labels_bool, labels_bool_bySession, labels_dict = roicat.tracking.clustering.make_label_variants(labels=labels, n_roi_bySession=data.n_roi)\n",
    "\n",
    "results_clusters = {\n",
    "    'labels': labels_squeezed,\n",
    "    'labels_bySession': labels_bySession,\n",
    "    'labels_dict': labels_dict,\n",
    "    'quality_metrics': quality_metrics,\n",
    "}\n",
    "\n",
    "results_all = {\n",
    "    \"clusters\":{\n",
    "        \"labels\": roicat.util.JSON_List(labels_squeezed),\n",
    "        \"labels_bySession\": roicat.util.JSON_List(labels_bySession),\n",
    "        \"labels_bool\": labels_bool,\n",
    "        \"labels_bool_bySession\": labels_bool_bySession,\n",
    "        \"labels_dict\": roicat.util.JSON_Dict(labels_dict),\n",
    "        \"quality_metrics\": roicat.util.JSON_Dict(clusterer.quality_metrics) if hasattr(clusterer, 'quality_metrics') else None,\n",
    "    },\n",
    "    \"ROIs\": {\n",
    "        \"ROIs_aligned\": aligner.ROIs_aligned,\n",
    "        \"ROIs_raw\": data.spatialFootprints,\n",
    "        \"frame_height\": data.FOV_height,\n",
    "        \"frame_width\": data.FOV_width,\n",
    "        \"idx_roi_session\": np.where(data.session_bool)[1],\n",
    "        \"n_sessions\": data.n_sessions,\n",
    "    },\n",
    "    \"input_data\": {\n",
    "        \"paths_stat\": data.paths_stat,\n",
    "        \"paths_ops\": data.paths_ops,\n",
    "    },\n",
    "}\n",
    "\n",
    "run_data = {\n",
    "    'data': data.__dict__,\n",
    "    'aligner': aligner.__dict__,\n",
    "    'blurrer': blurrer.__dict__,\n",
    "    'roinet': roinet.__dict__,\n",
    "    'swt': swt.__dict__,\n",
    "    'sim': sim.__dict__,\n",
    "    'clusterer': clusterer.__dict__,\n",
    "}\n",
    "\n",
    "params_used = {name: mod['params'] for name, mod in run_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f000941",
   "metadata": {},
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4384f2-e388-463f-9322-c7cb84948488",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of clusters: {len(np.unique(results_clusters[\"labels\"]))}')\n",
    "print(f'Number of discarded ROIs: {(np.array(results_clusters[\"labels\"])==-1).sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e812b",
   "metadata": {},
   "source": [
    "Look at some of the distributions of the quality metrics.\n",
    "- Silhouette score is a particularly useful one for this type of clustering. Learn more here: https://en.wikipedia.org/wiki/Silhouette_(clustering)\n",
    "- We also define a handy 'confidence' variable which is a nice heuristic you can use for thresholding for inclusion criteria\n",
    "- Note that the `sample_silhouette` score is a per-sample (per-ROI) score. So it can actually be used to remove / subselect ROIs from clusters.\n",
    "\n",
    "A good rule of thumb is to use an inclusion criteria of:\n",
    "- `sample_silhouette` > 0.1\n",
    "- `cluster_silhouette` > 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d012c5-ff39-4aa0-860b-cdd4e71dc0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.tracking.clustering.plot_quality_metrics(quality_metrics=quality_metrics, labels=labels_squeezed, n_sessions=data.n_sessions);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027846e",
   "metadata": {},
   "source": [
    "Look at a color visualization of the results. ROIs of the same color are considered a part of the same cluster. The colors are assigned randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe239a9-3e39-4990-812b-72a9f8c4c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV_clusters = roicat.visualization.compute_colored_FOV(\n",
    "    spatialFootprints=[r.power(1.0) for r in results_all['ROIs']['ROIs_aligned']],  ## Spatial footprint sparse arrays\n",
    "    FOV_height=results_all['ROIs']['frame_height'],\n",
    "    FOV_width=results_all['ROIs']['frame_width'],\n",
    "    labels=results_all[\"clusters\"][\"labels_bySession\"],  ## cluster labels\n",
    "#     labels=(np.array(results[\"clusters\"][\"labels\"])!=-1).astype(np.int64),  ## cluster labels\n",
    "    # alphas_labels=confidence*1.5,  ## Set brightness of each cluster based on some 1-D array\n",
    "#     alphas_labels=(clusterer.quality_metrics['cluster_silhouette'] > 0) * (clusterer.quality_metrics['cluster_intra_means'] > 0.4),\n",
    "#     alphas_sf=clusterer.quality_metrics['sample_silhouette'],  ## Set brightness of each ROI based on some 1-D array\n",
    ")\n",
    "\n",
    "roicat.visualization.display_toggle_image_stack(\n",
    "    FOV_clusters, \n",
    "    image_size=1.5,\n",
    "#     clim=[0,1.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ee60f",
   "metadata": {},
   "source": [
    "Visualize the images of ROIs from the same cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f0831-74f3-4530-ada7-33fe281e90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.visualization.display_cropped_cluster_ims(\n",
    "    spatialFootprints=results_all['ROIs']['ROIs_aligned'],\n",
    "    labels=np.array(results_all[\"clusters\"][\"labels\"]),\n",
    "    FOV_height=results_all['ROIs']['frame_height'],\n",
    "    FOV_width=results_all['ROIs']['frame_width'],\n",
    "    n_labels_to_display=10,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79a0c6",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3151ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the directory to save the results to\n",
    "dir_save = '/media/rich/bigSSD/data_tmp/test_data/'\n",
    "name_save = 'mouse_1'\n",
    "\n",
    "paths_save = {\n",
    "    'results_clusters': str(Path(dir_save) / f'{name_save}.tracking.results_clusters.json'),\n",
    "    'params_used':      str(Path(dir_save) / f'{name_save}.tracking.params_used.json'),\n",
    "    'results_all':      str(Path(dir_save) / f'{name_save}.tracking.results_all.richfile'),\n",
    "    'run_data':         str(Path(dir_save) / f'{name_save}.tracking.run_data.richfile'),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.json_save(obj=results_clusters, filepath=paths_save['results_clusters'])\n",
    "roicat.helpers.json_save(obj=params_used, filepath=paths_save['params_used'])\n",
    "roicat.util.RichFile_ROICaT(path=paths_save['results_all']).save(obj=results_all, overwrite=True)\n",
    "roicat.util.RichFile_ROICaT(path=paths_save['run_data']).save(obj=run_data, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433843e8",
   "metadata": {},
   "source": [
    "Optionally save the FOV_clusters images as a GIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a9f1c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.save_gif(\n",
    "    array=roicat.helpers.add_text_to_images(\n",
    "        images=[(f * 255).astype(np.uint8) for f in FOV_clusters], \n",
    "        text=[[f\"{ii}\",] for ii in range(len(FOV_clusters))], \n",
    "        font_size=3,\n",
    "        line_width=10,\n",
    "        position=(30, 90),\n",
    "    ), \n",
    "    path=str(Path(dir_save).resolve() / 'FOV_clusters.gif'),\n",
    "    frameRate=10.0,\n",
    "    loop=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562fe07",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f199a60b",
   "metadata": {},
   "source": [
    "##### Demo: Importing richfiles\n",
    "\n",
    "Simple output files are saved as .json files, which are easily handled. However, complex output files are saved using a custom format called `richfile`, which are basically just structured directories containing data files. This is a demonstration of how to import and handle richfile directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d08b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make a richfile object of the results_all dictionary\n",
    "r = roicat.util.RichFile_ROICaT(path=paths_save['results_all'])\n",
    "\n",
    "## You can load it\n",
    "temp_results_all = r.load()\n",
    "\n",
    "## You can load part of it by indexing into the richfile object\n",
    "print(f\"n_sessions: {r['ROIs']['n_sessions'].load()}\")\n",
    "print('')\n",
    "\n",
    "## You can view the tree structure\n",
    "r.view_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1790a",
   "metadata": {},
   "source": [
    "##### Legacy saving methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52333d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_save = Path('/media/rich/bigSSD/other_lab_data/Allen_institute/Jinho/roicat_results/721291/3').resolve()\n",
    "name_save = Path(dir_allOuterFolders).resolve().name\n",
    "\n",
    "path_save = dir_save / (name_save + '.ROICaT.tracking.results' + '.pkl')\n",
    "print(f'path_save: {path_save}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "225b9fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.pickle_save(\n",
    "    obj=results_all,\n",
    "    filepath=path_save,\n",
    "    mkdir=True,\n",
    ")\n",
    "\n",
    "roicat.helpers.pickle_save(\n",
    "    obj=run_data,\n",
    "    filepath=str(dir_save / (name_save + '.ROICaT.tracking.rundata' + '.pkl')),\n",
    "    mkdir=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81150d57",
   "metadata": {},
   "source": [
    "Optionally save results as a matlab file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "roicat.helpers.matlab_save(\n",
    "    obj=results_all,\n",
    "    filepath='/home/rich/Desktop/results_all.mat',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2595c913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
