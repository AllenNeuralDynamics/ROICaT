{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db3ae2f7",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97949d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conda Environment: ROI_env\n",
      "python version: 3.8.13\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "# check environment\n",
    "import os\n",
    "print(f'Conda Environment: ' + os.environ['CONDA_DEFAULT_ENV'])\n",
    "\n",
    "from platform import python_version\n",
    "print(f'python version: {python_version()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e36ab5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy Version: 1.21.6\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import importlib.util\n",
    "import glob\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from umap import UMAP\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit, StratifiedShuffleSplit\n",
    "import scipy.stats\n",
    "import scipy.signal\n",
    "from kymatio import Scattering2D\n",
    "import json\n",
    "import torchvision\n",
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import h5py\n",
    "# import figgen as fg\n",
    "import sys\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "print('Numpy Version:', np.__version__)\n",
    "# print('TorchVision Version:',torchvision.__version__)\n",
    "\n",
    "dir_github = Path(r'/n/data1/hms/neurobio/sabatini/josh/github_repos/').resolve()\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(dir_github))\n",
    "\n",
    "from ROICaT.roicat import helpers, ROInet\n",
    "# from ROICaT.roicat.tracking import data_importing, visualization, alignment, blurring, ROInet, scatteringWaveletTransformer, similarity_graph, clustering\n",
    "from ROICaT.roicat.classification import data, preprocess, classify, evaluate, visualize\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72274363",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "toc = {}\n",
    "toc['start'] = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8673d43",
   "metadata": {},
   "source": [
    "# Import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c375e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spr(*directory_list):\n",
    "    for dir_num, directory in enumerate(directory_list):\n",
    "        if dir_num == 0:\n",
    "            full_directory = Path(directory)\n",
    "        else:\n",
    "            full_directory = full_directory / directory\n",
    "    return str(full_directory.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f6d2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_list(l):\n",
    "    for item in l:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc7d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_allOuterFolders = Path(r\"/media/rich/bigSSD/other lab data/Harnett_lab/ROI_Tracking/Vincent_Valerio/4th_email/AllStatFiles/rbp16\").resolve()\n",
    "# # dir_allOuterFolders = Path(r\"/media/rich/bigSSD/res2p/scanimage data/round 5 experiments/mouse 2_6/just_stat_files\").resolve()\n",
    "\n",
    "# folders_allSessions = natsort.natsorted(helpers.get_dir_contents(dir_allOuterFolders)[0])\n",
    "\n",
    "# dir_allS2pFolders = [dir_allOuterFolders / folder for folder in folders_allSessions]\n",
    "\n",
    "# pathSuffixToStat = 'plane0/stat.npy'\n",
    "# pathSuffixToOps = 'plane0/ops.npy'\n",
    "# # pathSuffixToStat = 'stat.npy'\n",
    "# # pathSuffixToOps = 'ops.npy'\n",
    "\n",
    "# paths_allStat = np.array([path / pathSuffixToStat for path in dir_allS2pFolders])[:]\n",
    "# paths_allOps  = np.array([path / pathSuffixToOps for path in dir_allS2pFolders])[:]\n",
    "# # paths_allStat = np.array([path / pathSuffixToStat for path in dir_allS2pFolders])\n",
    "# # paths_allOps  = np.array([path / pathSuffixToOps for path in dir_allS2pFolders])\n",
    "\n",
    "# print(f'folder names of all sessions: \\n{folders_allSessions}')\n",
    "# print(f'paths to all stat files: \\n{paths_allStat}')\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# || Specify Raw Data Filename Sources ||\n",
    "# ---------------------------------------\n",
    "stat_files = spr('/n/data1/hms/neurobio/sabatini/josh/github_repos/GCaMP_ROI_classifier/data/training/mouse2_6__20210409/stat.npy')\n",
    "label_files = spr('/n/data1/hms/neurobio/sabatini/josh/github_repos/GCaMP_ROI_classifier/data/training/mouse2_6__20210409/labels_round2_sesh2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01ca7b",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4179f95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting: Importing spatial footprints from stat files\n",
      "Completed: Imported 4898 stat files into class as self.statFiles. Total number of ROIs: 4898. Number of ROI from each file: [4898]\n",
      "Starting: Importing labels footprints from npy files\n",
      "Completed: Imported 4898 labels into class as self.labelFiles. Total number of ROIs: 4898. Number of ROI from each file: [4898]\n"
     ]
    }
   ],
   "source": [
    "dat = data.Data(\n",
    "    paths_statFiles=stat_files,\n",
    "    paths_labelFiles=label_files,\n",
    "    um_per_pixel=1.0,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "dat.import_statFiles();\n",
    "dat.import_labelFiles();\n",
    "\n",
    "# data.import_ROI_spatialFootprints(workers=-1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8363c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "# # From ROICaT\n",
    "# visualization.display_toggle_image_stack(np.concatenate(data.ROI_images, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5a179d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc['import_data'] = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83365f09",
   "metadata": {},
   "source": [
    "# Concatenate / Adjust / Clean Data + Drop Non-Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff4c6f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.drop_nan_rois();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232dd31e",
   "metadata": {},
   "source": [
    "# Neural Network Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ea0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful hash comparison. Found matching files: {'params': '/n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/params.json', 'model': '/n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/model.py', 'state_dict': '/n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/ConvNext_tiny__1_0_best__simCLR.pth'}\n",
      "Imported model from /n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/model.py\n",
      "Loaded params_model from /n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/params.json\n",
      "Generated network using params_model\n",
      "Loaded state_dict into network from /n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/ConvNext_tiny__1_0_best__simCLR.pth\n",
      "Loaded network onto device cuda:0\n",
      "Starting: resizing ROIs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.21it/s]\n",
      "/home/joz608/.conda/envs/ROI_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 48 worker processes in total. Our suggested max number of worker in current system is 16, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: resizing ROIs\n",
      "Defined image transformations: Sequential(\n",
      "  (0): ScaleDynamicRange(scaler_bounds=(0, 1))\n",
      "  (1): Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "  (2): TileChannels(dim=0)\n",
      ")\n",
      "Defined dataset\n",
      "Defined dataloader\n",
      "starting: running data through network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 613/613 [00:12<00:00, 48.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed: running data through network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hash_dict_true = {\n",
    "    'params': ('params.json', '68cf1bd47130f9b6d4f9913f86f0ccaa'),\n",
    "    'model': ('model.py', '61c85529b7aa33e0dfadb31ee253a7e1'),\n",
    "    'state_dict': ('ConvNext_tiny__1_0_best__simCLR.pth', '3287e001ff28d07ada2ae70aa7d0a4da'),\n",
    "}\n",
    "\n",
    "roinet = ROInet.ROInet_embedder(\n",
    "    device='cuda:0',\n",
    "#     dir_networkFiles='/home/rich/Downloads/ROInet',\n",
    "    dir_networkFiles='/n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT',\n",
    "    \n",
    "#     download_from_gDrive='force_download',\n",
    "#     download_from_gDrive='force_local',\n",
    "    download_from_gDrive='check_local_first',\n",
    "    gDriveID='1D2Qa-YUNX176Q-wgboGflW0K6un7KYeN',\n",
    "    hash_dict_networkFiles=hash_dict_true,\n",
    "#     hash_dict_networkFiles=None,\n",
    "    forward_pass_version='latent',\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "roinet.generate_dataloader(\n",
    "    ROI_images=[dat.statFiles],\n",
    "    um_per_pixel=dat.um_per_pixel,\n",
    "    pref_plot=False,\n",
    "    batchSize_dataloader=8,\n",
    "    pinMemory_dataloader=True,\n",
    "    numWorkers_dataloader=mp.cpu_count(),\n",
    "    persistentWorkers_dataloader=True,\n",
    "    prefetchFactor_dataloader=2,    \n",
    ");\n",
    "\n",
    "roinet.generate_latents();\n",
    "\n",
    "# roinet.latents\n",
    "# roinet.dataset\n",
    "# roinet.net\n",
    "# roinet.params_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b14446a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = (roinet.latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c19c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load('/n/data1/hms/neurobio/sabatini/josh/analysis/data_backup_save/latents.npy')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c20ef0c",
   "metadata": {},
   "source": [
    "#         raw_images = roinet.ROI_images_rs\n",
    "#         raw_images_dup = np.concatenate([raw_images for _ in range(len(raw_labels_match))], axis=0)\n",
    "\n",
    "#         self.latents = roinet.latents\n",
    "#         latents_dup = np.concatenate([latents for _ in range(len(raw_labels_match))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1419d5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb8a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc['NN'] = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6500d0",
   "metadata": {},
   "source": [
    "# UMAP Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d211e599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting UMAP...\n",
      "Generating Embeddings...\n",
      "Embeddings Generated...\n"
     ]
    }
   ],
   "source": [
    "umap_params = dict(\n",
    "    n_neighbors=30,\n",
    "    n_components=2,\n",
    "    metric='euclidean',\n",
    "    metric_kwds=None,\n",
    "    output_metric='euclidean',\n",
    "    output_metric_kwds=None,\n",
    "    n_epochs=None,\n",
    "    learning_rate=1.0,\n",
    "    init='spectral',\n",
    "    min_dist=0.1,\n",
    "    spread=1.0,\n",
    "    low_memory=True,\n",
    "    n_jobs=-1,\n",
    "    set_op_mix_ratio=1.0,\n",
    "    local_connectivity=1.0,\n",
    "    repulsion_strength=1.0,\n",
    "    negative_sample_rate=5,\n",
    "    transform_queue_size=4.0,\n",
    "    a=None,\n",
    "    b=None,\n",
    "    random_state=None,\n",
    "    angular_rp_forest=False,\n",
    "    target_n_neighbors=-1,\n",
    "    target_metric='categorical',\n",
    "    target_metric_kwds=None,\n",
    "    target_weight=0.5,\n",
    "    transform_seed=42,\n",
    "    transform_mode='embedding',\n",
    "    force_approximation_algorithm=False,\n",
    "    verbose=False,\n",
    "    tqdm_kwds=None,\n",
    "    unique=False,\n",
    "    densmap=False,\n",
    "    dens_lambda=2.0,\n",
    "    dens_frac=0.3,\n",
    "    dens_var_shift=0.1,\n",
    "    output_dens=False,\n",
    "    disconnection_distance=None,\n",
    "    precomputed_knn=(None, None, None),\n",
    ")\n",
    "\n",
    "umap = UMAP(**umap_params)\n",
    "print('Fitting UMAP...')\n",
    "umap.fit(latents)\n",
    "print('Generating Embeddings...')\n",
    "embeddings = umap.transform(latents)\n",
    "print('Embeddings Generated...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d4fad",
   "metadata": {},
   "source": [
    "# Visualize Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a46b796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 128]), (4898,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roinet.latents.shape, dat.labelFiles.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6072f1a",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "%matplotlib notebook\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], s=5, c=dat.labelFiles[:], cmap='gist_rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fd1e05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc['visualize'] = time.time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f0c10",
   "metadata": {},
   "source": [
    "# Load / Setup Classifier — ZScore + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8c1489ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA()),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(C=1, class_weight=&#x27;balanced&#x27;,\n",
       "                                    max_iter=10000))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA()),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(C=1, class_weight=&#x27;balanced&#x27;,\n",
       "                                    max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=1, class_weight=&#x27;balanced&#x27;, max_iter=10000)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()), ('pca', PCA()),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1, class_weight='balanced',\n",
       "                                    max_iter=10000))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprocessor = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(),\n",
    "    LogisticRegression(\n",
    "        solver='lbfgs',\n",
    "        fit_intercept=True, \n",
    "        class_weight='balanced',\n",
    "        max_iter=10000,\n",
    "        C=1\n",
    "    )\n",
    ")\n",
    "preprocessor.fit(latents, dat.labelFiles);\n",
    "\n",
    "proba = preprocessor.predict_proba(latents);\n",
    "preds = np.argmax(proba, axis=1);\n",
    "cm = helpers.confusion_matrix(preds, dat.labelFiles.astype(np.int32), counts=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "173c10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1294a171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pp \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform_preprocess(latents)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'pipeline'"
     ]
    }
   ],
   "source": [
    "preprocessor = preprocess.Preprocessor()\n",
    "pp = preprocessor.fit_transform_preprocess(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853ae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.transform_preprocess(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = classify.Classifier(preprocessor)\n",
    "classifier.fit_classifier(latents, dat.labelFiles);\n",
    "proba, preds = classifier.classify(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c08acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = evaluate.Evaluation(classifier)\n",
    "cm = evaluator.confusion_matrix(latents, dat.labelFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1285c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "sns.heatmap(cm, annot=True, annot_kws={\"size\": 16}, vmax=1., cmap=plt.get_cmap('gray'), ax=ax)\n",
    "fig.suptitle('Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49aa93",
   "metadata": {},
   "source": [
    "## Train/Holdout Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfe126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af582cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train/Holdout Split\n",
    "# features_train, features_holdout, labels_train, labels_holdout = sklearn.model_selection.train_test_split(features_norm, labels_clean[:features_norm.shape[0]], test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cd712",
   "metadata": {},
   "source": [
    "## Shuffle Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c114a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb025cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle Split\n",
    "# splitter = ShuffleSplit(n_splits=classifier_n_splits)\n",
    "# all_split_inx = list(splitter.split(features_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f17b9d",
   "metadata": {},
   "source": [
    "## Train'/Val Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfd57c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Train'/Val Extract\n",
    "# trainp_X = [features_train[_[0]] for _ in all_split_inx]\n",
    "# val_X = [features_train[_[1]] for _ in all_split_inx]\n",
    "# trainp_y = [labels_train[_[0]] for _ in all_split_inx]\n",
    "# val_y = [labels_train[_[1]] for _ in all_split_inx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc5ea7",
   "metadata": {},
   "source": [
    "## Fit Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c727450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sweep through regularizations for classifier\n",
    "# for ic, c in enumerate(C_toUse):\n",
    "#     cm_trp_lst = []\n",
    "#     cm_val_lst = []\n",
    "\n",
    "#     cm_trp_cnt_lst = []\n",
    "#     cm_val_cnt_lst = []\n",
    "\n",
    "#     acc_trp_lst = []\n",
    "#     acc_val_lst = []\n",
    "\n",
    "#     # Sweep Through Shuffle Splits\n",
    "#     # trp = Train' or train within single Shuffle Split fold\n",
    "#     # val = Validation or test set within single Shuffle Split fold\n",
    "#     for inx_split in trange(len(trainp_X)):\n",
    "#         tmp_trainp_X = trainp_X[inx_split]\n",
    "#         tmp_trainp_y = trainp_y[inx_split]\n",
    "\n",
    "#         tmp_val_X = val_X[inx_split]\n",
    "#         tmp_val_y = val_y[inx_split]\n",
    "\n",
    "#         logreg = fg.fit_classifier_logreg(tmp_trainp_X, tmp_trainp_y, max_iter=logistic_max_iter, C=c)\n",
    "\n",
    "#         cm_trp = fg.cm_classifier_logreg(logreg, tmp_trainp_X, tmp_trainp_y, github_loc=fig_gen_params['github_loc'])\n",
    "#         cm_val = fg.cm_classifier_logreg(logreg, tmp_val_X, tmp_val_y, github_loc=fig_gen_params['github_loc'])\n",
    "\n",
    "#         cm_trp_cnt = fg.cm_classifier_logreg(logreg, tmp_trainp_X, tmp_trainp_y, counts=True, github_loc=fig_gen_params['github_loc'])\n",
    "#         cm_val_cnt = fg.cm_classifier_logreg(logreg, tmp_val_X, tmp_val_y, counts=True, github_loc=fig_gen_params['github_loc'])\n",
    "\n",
    "#         acc_trp = fg.score_classifier_logreg(logreg, tmp_trainp_X, tmp_trainp_y)\n",
    "#         acc_val = fg.score_classifier_logreg(logreg, tmp_val_X, tmp_val_y)\n",
    "\n",
    "#         cm_trp_lst.append(cm_trp)\n",
    "#         cm_val_lst.append(cm_val)\n",
    "\n",
    "#         cm_trp_cnt_lst.append(cm_trp_cnt)\n",
    "#         cm_val_cnt_lst.append(cm_val_cnt)\n",
    "\n",
    "#         acc_trp_lst.append(acc_trp)\n",
    "#         acc_val_lst.append(acc_val)\n",
    "    \n",
    "    \n",
    "#     cm_trp_mn = np.mean(cm_trp_lst,axis=0)\n",
    "#     cm_val_mn = np.mean(cm_val_lst,axis=0)\n",
    "\n",
    "#     cm_trp_cnt_sm = np.sum(cm_trp_cnt_lst,axis=0)\n",
    "#     cm_val_cnt_sm = np.sum(cm_val_cnt_lst,axis=0)\n",
    "\n",
    "#     # Refitting model to all of training / CV data and evaluating on heldout data\n",
    "#     logreg_refit = fg.fit_classifier_logreg(features_train, labels_train, max_iter=logistic_max_iter, C=c)\n",
    "\n",
    "#     cm_tr = fg.cm_classifier_logreg(logreg_refit, features_train, labels_train, counts=False, github_loc=fig_gen_params['github_loc'])\n",
    "#     cm_ho = fg.cm_classifier_logreg(logreg_refit, features_holdout, labels_holdout, counts=False, github_loc=fig_gen_params['github_loc'])\n",
    "\n",
    "#     cm_tr_cnt = fg.cm_classifier_logreg(logreg_refit, features_train, labels_train, counts=True, github_loc=fig_gen_params['github_loc'])\n",
    "#     cm_ho_cnt = fg.cm_classifier_logreg(logreg_refit, features_holdout, labels_holdout, counts=True, github_loc=fig_gen_params['github_loc'])\n",
    "\n",
    "#     acc_tr = fg.score_classifier_logreg(logreg_refit, features_train, labels_train)\n",
    "#     acc_ho = fg.score_classifier_logreg(logreg_refit, features_holdout, labels_holdout)\n",
    "    \n",
    "#     restricted_n_train_results = {}\n",
    "#     for n_train in n_train_to_use:\n",
    "#         print('Saving: n_train',n_train)\n",
    "#         train_size = n_train/features_train.shape[0] if type(n_train) == type(None) and n_train < features_train else None\n",
    "#         print('Saving: train_size',train_size)\n",
    "#         if train_size is not None:\n",
    "#             # Refitting model to n_train data points from training data / and evaluating on heldout data\n",
    "#             sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size)\n",
    "#             train_subset_inx, _ = sss.split(features_train, labels_train)[0]\n",
    "#             features_train_subset, labels_train_subset = features_train[train_subset_inx], labels_train[train_subset_inx]\n",
    "            \n",
    "#             logreg_refit = fg.fit_classifier_logreg(features_train_subset, labels_train_subset, max_iter=logistic_max_iter, C=c)\n",
    "            \n",
    "#             restricted_n_train_results[f'acc_tr_subset_{n_train}'] = fg.score_classifier_logreg(logreg_refit, features_train_subset, labels_train_subset)\n",
    "#             restricted_n_train_results[f'acc_tr_{n_train}'] = fg.score_classifier_logreg(logreg_refit, features_train, labels_train)\n",
    "#             restricted_n_train_results[f'acc_ho_{n_train}'] = fg.score_classifier_logreg(logreg_refit, features_holdout, labels_holdout)\n",
    "    \n",
    "#     with h5py.File(fig_gen_params['h5_out_name'], 'a') as f:\n",
    "#         g = f.create_group(f'creg_{c}')\n",
    "#         g.attrs['acc_tr'] = np.mean(acc_tr)\n",
    "#         g.attrs['acc_trp'] = np.mean(acc_trp_lst)\n",
    "#         g.attrs['acc_val'] = np.mean(acc_val_lst)\n",
    "#         g.attrs['acc_ho'] = np.mean(acc_ho)\n",
    "        \n",
    "#         for n_train in n_train_to_use:\n",
    "#             print('Saving: n_train',n_train)\n",
    "#             train_size = n_train/features_train.shape[0] if type(n_train) == type(None) and n_train < features_train else None\n",
    "#             print('Saving: train_size',train_size)\n",
    "#             if train_size is not None:\n",
    "#                 g.attrs[f'acc_tr_subset_{n_train}'] = restricted_n_train_results[f'acc_tr_subset_{n_train}']\n",
    "#                 g.attrs[f'acc_tr_{n_train}'] = restricted_n_train_results[f'acc_tr_{n_train}']\n",
    "#                 g.attrs[f'acc_ho_{n_train}'] = restricted_n_train_results[f'acc_ho_{n_train}']\n",
    "\n",
    "#         gg = g.create_group(f'cm_prc')\n",
    "\n",
    "#         gg.create_dataset(f'tr', data=cm_tr)\n",
    "#         gg.create_dataset(f'trp', data=cm_trp_mn)\n",
    "#         gg.create_dataset(f'val', data=cm_val_mn)\n",
    "#         gg.create_dataset(f'ho', data=cm_ho)\n",
    "\n",
    "#         gg = g.create_group(f'cm_count')\n",
    "\n",
    "#         gg.create_dataset(f'tr', data=cm_tr_cnt)\n",
    "#         gg.create_dataset(f'trp', data=cm_trp_cnt_sm)\n",
    "#         gg.create_dataset(f'val', data=cm_val_cnt_sm)\n",
    "#         gg.create_dataset(f'ho', data=cm_ho_cnt)\n",
    "\n",
    "#         gg = g.create_group(f'logreg_model')\n",
    "\n",
    "#         gg.create_dataset(f'coef', data=logreg_refit.coef_)\n",
    "#         gg.create_dataset(f'int', data=logreg_refit.intercept_)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f193b",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir_save = Path('/home/rich/Desktop/').resolve()\n",
    "dir_save = Path('/n/data1/hms/neurobio/sabatini/josh/analysis/roinet-paper/ROICaT/output/').resolve()\n",
    "name_save = dir_allOuterFolders.name\n",
    "path_save = dir_save / (name_save + '.ROICaT.results' + '.pkl')\n",
    "\n",
    "ROIs = {\n",
    "    \"ROIs_aligned\": aligner.ROIs_aligned,\n",
    "    \"ROIs_raw\": dat.spatialFootprints,\n",
    "    \"frame_height\": dat.FOV_height,\n",
    "    \"frame_width\": dat.FOV_width,\n",
    "    \"idx_roi_session\": np.where(dat.sessionID_concat)[1]\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"UCIDs\": labels,\n",
    "    \"UCIDs_bySession\": labels_bySession,\n",
    "    \"ROIs\": ROIs,\n",
    "}\n",
    "\n",
    "helpers.simple_save(\n",
    "    obj=results,\n",
    "    filename=path_save,\n",
    "    mkdir=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a7b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc['saving'] = time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407b108",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1143fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep through regularizations for classifier\n",
    "\n",
    "    # Sweep Through Shuffle Splits\n",
    "\n",
    "    # Refitting model to all of training / CV data and evaluating on heldout data\n",
    "\n",
    "            # Refitting model to n_train data points from training data / and evaluating on heldout data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
